{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/disheng/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.53it/s]\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "pipe(text=messages)\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=1000):\n",
        "    \"\"\"\n",
        "    安全加载图片，自动检查、降分辨率。\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): 图片路径\n",
        "        max_size (int): 模型支持的最大分辨率 (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB 图片，已 resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"❌ Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # 如果图片太大则等比缩放\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # 等比缩放，最大边= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EgoOrientBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_base_path = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/\"\n",
        "json_file = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/benchmark.json\"\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "import os\n",
        "\n",
        "def evaluate_spaceom(pipe, data):\n",
        "    acc = 0\n",
        "    total = len(data)\n",
        "    results = []\n",
        "    for item in tqdm(data):\n",
        "        image_path = f\"{image_base_path}/{item['image']}\"\n",
        "        # img = Image.open(image_path).convert(\"RGB\")\n",
        "        img = load_and_resize_image(image_path, max_size=448)  # Ensure image is resized correctly\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": item[\"question\"]}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        response = pipe(text=messages)\n",
        "        model_answer = response[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "        results.append({\n",
        "            \"image\": item[\"image\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"answer\": model_answer,\n",
        "            \"label\": item[\"label\"],\n",
        "            \"Accuracy\": item[\"label\"] in model_answer\n",
        "        })\n",
        "    #save results to a JSON file\n",
        "    with open(\"EgoOrientBench_spaceom_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Accuracy: {sum(1 for r in results if r['Accuracy']) / total * 100:.2f}%\")\n",
        "    print(f\"Total: {total}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 9/33460 [00:01<1:45:24,  5.29it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  7%|▋         | 2260/33460 [06:17<1:24:38,  6.14it/s]../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n",
            "  7%|▋         | 2260/33460 [06:17<1:26:56,  5.98it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_spaceom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mevaluate_spaceom\u001b[39m\u001b[34m(pipe, data)\u001b[39m\n\u001b[32m     17\u001b[39m img = load_and_resize_image(image_path, max_size=\u001b[32m448\u001b[39m)  \u001b[38;5;66;03m# Ensure image is resized correctly\u001b[39;00m\n\u001b[32m     19\u001b[39m messages = [\n\u001b[32m     20\u001b[39m     {\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     },\n\u001b[32m     27\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m response = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m model_answer = response[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m     30\u001b[39m results.append({\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m model_answer\n\u001b[32m     36\u001b[39m })\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/image_text_to_text.py:339\u001b[39m, in \u001b[36mImageTextToTextPipeline.__call__\u001b[39m\u001b[34m(self, images, text, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_chat(text):\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    341\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1457\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1458\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1461\u001b[39m         )\n\u001b[32m   1462\u001b[39m     )\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1471\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1470\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1371\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1370\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/image_text_to_text.py:425\u001b[39m, in \u001b[36mImageTextToTextPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, generate_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m prompt_text = model_inputs.pop(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m input_ids = (\n\u001b[32m    423\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_inputs \u001b[38;5;28;01melse\u001b[39;00m model_inputs[\u001b[33m\"\u001b[39m\u001b[33mdecoder_input_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    424\u001b[39m )  \u001b[38;5;66;03m# for decoder-only models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m generated_sequence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mgenerated_sequence\u001b[39m\u001b[33m\"\u001b[39m: generated_sequence, \u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m: prompt_text, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: input_ids}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:3597\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3595\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3597\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3598\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   3599\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   3601\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._has_unfinished_sequences\u001b[39m\u001b[34m(self, this_peer_finished, synced_gpus, device)\u001b[39m\n\u001b[32m   2782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag.item() == \u001b[32m0.0\u001b[39m:\n\u001b[32m   2783\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[32m   2785\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "evaluate_spaceom(pipe, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GeoMeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 18.14it/s]\n",
            "Device set to use cuda:2\n",
            "Inference:  10%|█         | 10/100 [00:02<00:19,  4.52it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Inference: 100%|██████████| 100/100 [00:18<00:00,  5.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶️ Overall Accuracy: 43.00%\n",
            "✅ 结果已保存到 geobench_real_results_with_predictions.{json,csv}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# 🚀 1) 环境准备\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# %%\n",
        "# ⚙️ 2) 参数配置\n",
        "IMAGE_BASE   = Path(\"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/\")\n",
        "JSONL_FILE   = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/depth_height_1000_realworld.jsonl\"\n",
        "MODEL_NAME   = \"remyxai/SpaceOm\"\n",
        "TASK         = \"image-text-to-text\"\n",
        "DEVICE_ID    = 2      # 单卡就用 0\n",
        "BATCH_SIZE   = 1     # 根据显存调\n",
        "\n",
        "# %%\n",
        "# 📖 3) 读数据\n",
        "data = []\n",
        "with open(JSONL_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        assert \"images\" in item and \"query_text\" in item and \"target_text\" in item\n",
        "        data.append(item)\n",
        "\n",
        "# %%\n",
        "# 🔧 4) 初始化 pipeline\n",
        "pipe = pipeline(\n",
        "    TASK,\n",
        "    model=MODEL_NAME,\n",
        "    device=DEVICE_ID,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# 🏃 5) 批量推理 + 进度条\n",
        "results = []\n",
        "for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Inference\"):\n",
        "    batch = data[i : i + BATCH_SIZE]\n",
        "    messages = []\n",
        "    for item in batch:\n",
        "        img = Image.open(IMAGE_BASE / item[\"images\"][0]).convert(\"RGB\")\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\",  \"text\":  item[\"query_text\"]}\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    outputs = pipe(text=messages)\n",
        "\n",
        "    for item, out in zip(batch, outputs):\n",
        "        # 取 generated_text 这个 list，然后找出 assistant 那条\n",
        "        gen_list = out.get(\"generated_text\", [])\n",
        "        assistant_entry = next(\n",
        "            (entry for entry in gen_list if entry.get(\"role\")==\"assistant\"),\n",
        "            None\n",
        "        )\n",
        "        if assistant_entry is None:\n",
        "            # 万一没找到，就降级处理\n",
        "            raw_text = str(gen_list)\n",
        "        else:\n",
        "            raw_text = assistant_entry.get(\"content\", \"\")\n",
        "\n",
        "        # 用正则去掉末尾多余的逗号、句号\n",
        "        pred = re.sub(r\"[，,\\.。]+$\", \"\", raw_text).strip()\n",
        "\n",
        "        results.append({\n",
        "            \"image\": item[\"images\"][0],\n",
        "            \"query\": item[\"query_text\"],\n",
        "            \"pred\":  pred,\n",
        "            \"gold\":  item[\"target_text\"]\n",
        "        })\n",
        "\n",
        "# %%\n",
        "# 📊 6) 计算准确率 & 保存\n",
        "df = pd.DataFrame(results)\n",
        "accuracy = (df[\"pred\"] == df[\"gold\"]).mean()\n",
        "print(f\"▶️ Overall Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "df.to_json(\"geobench_real_results_with_predictions.json\",\n",
        "           orient=\"records\", indent=2, force_ascii=False)\n",
        "df.to_csv(\"geobench_real_results_with_predictions.csv\", index=False)\n",
        "print(\"✅ 结果已保存到 geobench_real_results_with_predictions.{json,csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OmniSpatial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
        "!mkdir -p dataset\n",
        "!huggingface-cli download --resume-download qizekun/OmniSpatial --local-dir dataset --repo-type dataset\n",
        "!find dataset/ -name '*.zip' -exec unzip -o {} -d dataset/ \\;\n",
        "!rm -f dataset/*.zip && rm -rf dataset/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=448):\n",
        "    \"\"\"\n",
        "    安全加载图片，自动检查、降分辨率。\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): 图片路径\n",
        "        max_size (int): 模型支持的最大分辨率 (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB 图片，已 resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"❌ Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # 如果图片太大则等比缩放\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # 等比缩放，最大边= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 13.64it/s]\n",
            "Device set to use cuda:0\n",
            "  1%|          | 10/1533 [00:01<03:29,  7.27it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 36%|███▌      | 551/1533 [01:47<05:12,  3.14it/s]/home/disheng/miniconda3/lib/python3.12/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "100%|██████████| 1533/1533 [04:14<00:00,  6.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "# pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "question_types =  {0:\"Dynamic_Reasoning\", \n",
        "                   1:\"Spatial_Interaction\", \n",
        "                   2:\"Complex_Logic\", \n",
        "                   3:\"Perspective_Taking\"}\n",
        "annotation = \"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/data.json\"\n",
        "import json\n",
        "annotation_data = json.load(open(annotation, \"r\"))\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "record = {}\n",
        "for item in tqdm(annotation_data):\n",
        "    iid = item[\"id\"]\n",
        "    image_id = item[\"id\"].split(\"_\")[0]\n",
        "    question_type = item[\"task_type\"]\n",
        "\n",
        "    question = item[\"question\"]\n",
        "    options = item[\"options\"]\n",
        "    full_prompt = f\"Question: {question}\\nOptions: {\"; \".join(options)}. Please only retuen a correct option without analysis.\"\n",
        "    label = item[\"answer\"]\n",
        "    if len(options) != 0:\n",
        "        label = options[label]\n",
        "    image_path = f\"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/{question_type}/{image_id}.png\"\n",
        "    img = load_and_resize_image(image_path, max_size=448)  # 使用安全加载函数\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {iid}: {e}\")\n",
        "        print(image_path)\n",
        "        print(options)\n",
        "        print(full_prompt)\n",
        "        print(label)\n",
        "        print(messages)\n",
        "        print()\n",
        "        continue\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    # record all of information for later analysis\n",
        "    record[iid] = {\n",
        "        \"image_path\": image_path,\n",
        "        \"question_type\": question_type,\n",
        "        \"question\": question,\n",
        "        \"options\": options,\n",
        "        \"label\": label,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": model_answer == label\n",
        "    }\n",
        "# 保存结果\n",
        "import json\n",
        "with open(\"omnispatial_results.json\", \"w\") as f:\n",
        "    json.dump(record, f, indent=2, ensure_ascii=False)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 43.70%\n"
          ]
        }
      ],
      "source": [
        "result = \"/home/disheng/Spatial_Survey/omnispatial_results.json\"\n",
        "import json\n",
        "record = json.load(open(result, \"r\"))\n",
        "acc = 0\n",
        "total = len(record)\n",
        "correct ={}\n",
        "wrong = {}\n",
        "for id, item in record.items():\n",
        "    label = item[\"label\"]\n",
        "    model_answer = item[\"model_answer\"]\n",
        "    if label in model_answer:\n",
        "        acc += 1\n",
        "        correct[id] = item\n",
        "    else:\n",
        "        wrong[id] = item\n",
        "accuracy = acc / total\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VSI-Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'No'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"nyu-visionx/VSI-Bench\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SpatialRGPT-Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"a8cheng/SpatialRGPT-Bench\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
